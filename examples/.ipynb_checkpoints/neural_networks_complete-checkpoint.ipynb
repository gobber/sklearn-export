{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting more complex models\n",
    "\n",
    "This file contains examples of exporting neural networks from `sklearn` library.\n",
    "\n",
    "### Loading common packages\n",
    "Instaling the `sklearn-export` using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sklearn_export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading common packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:51.245481Z",
     "start_time": "2021-04-20T17:27:51.241187Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_diabetes\n",
    "from sklearn_export import Export\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting a neural network classifier\n",
    "\n",
    "Let us export a more complex model (a neural network for classification) including normalization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:52.038492Z",
     "start_time": "2021-04-20T17:27:52.033367Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.neural_network.multilayer_perceptron module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neural_network. Anything that cannot be imported from sklearn.neural_network is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# To support old versions of sklearn\n",
    "sklearn_ver = Export.take_sklearn_version()\n",
    "if sklearn_ver[:2] >= (0, 18):\n",
    "    if sklearn_ver[:2] < (0, 24):\n",
    "        from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
    "    else:\n",
    "        from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firtly, let us load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:52.466640Z",
     "start_time": "2021-04-20T17:27:52.456616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data and spliting in train and test sets\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us normalize the dataset using the Standard Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:52.847445Z",
     "start_time": "2021-04-20T17:27:52.841498Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "Xz_train = scaler.fit_transform(X_train)\n",
    "Xz_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a neural network for classification using a simple shallow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:53.675312Z",
     "start_time": "2021-04-20T17:27:53.217610Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(max_iter=10000)\n",
    "clf = clf.fit(Xz_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we export the model using `sklearn-export`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:53.684808Z",
     "start_time": "2021-04-20T17:27:53.678197Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.svm.classes module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.svm. Anything that cannot be imported from sklearn.svm is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "export = Export([scaler, clf])\n",
    "export.to_json(filename='neural_net.json');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the data from te JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:53.944925Z",
     "start_time": "2021-04-20T17:27:53.938898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('neural_net.json')\n",
    "\n",
    "# Transforming in a dict\n",
    "model_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us reuse our standard scaler method from the scaler example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:54.294564Z",
     "start_time": "2021-04-20T17:27:54.289503Z"
    }
   },
   "outputs": [],
   "source": [
    "# An example of a standard scaler implemented with model_data\n",
    "def standard_scaler(X, model_data):\n",
    "    mean = np.asarray(model_data['mean'])\n",
    "    std = np.asarray(model_data['std'])\n",
    "    Xz = (X-mean)/std\n",
    "    return Xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict new values using a trained neural network we need to perform the foward propagation step. It is just a composition of functions (linear combinations followed by activations). The last composition gives the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:54.665686Z",
     "start_time": "2021-04-20T17:27:54.645798Z"
    }
   },
   "outputs": [],
   "source": [
    "# An implementation of the forward propagation process in python numpy\n",
    "def forward_propagation(X, model_data, threshold=0.5):\n",
    "    \n",
    "    # Valid activations in sklearn neural network models\n",
    "    sigmoid = lambda z: 1/(1 + np.exp(-z))\n",
    "    softmax = lambda z: np.exp(z)/np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    relu = lambda z: np.maximum(z, 0)\n",
    "    tanh = lambda z: np.tanh(z)\n",
    "    identity = lambda z: z\n",
    "    \n",
    "    output_activation = None\n",
    "    hidden_activation = None\n",
    "    \n",
    "    # Output activation\n",
    "    if model_data['output_activation'] == 'logistic':\n",
    "        output_activation = sigmoid\n",
    "    elif model_data['output_activation'] == 'softmax':\n",
    "        output_activation = softmax\n",
    "    else:\n",
    "        output_activation = identity        \n",
    "    \n",
    "    # Hidden activation\n",
    "    if model_data['hidden_activation'] == 'sigmoid':\n",
    "        hidden_activation = sigmoid\n",
    "    elif model_data['hidden_activation'] == 'relu':\n",
    "        hidden_activation = relu\n",
    "    elif model_data['hidden_activation'] == 'tanh':\n",
    "        hidden_activation = tanh\n",
    "    else:\n",
    "        hidden_activation = identity\n",
    "    \n",
    "    # Fist layer (is the input data itself)\n",
    "    a = X\n",
    "    \n",
    "    for l in range(len(model_data['weights'])-1):        \n",
    "        W = np.asarray(model_data['weights'][l]).reshape(model_data['numRows'][l], model_data['numColumns'][l], order='F')\n",
    "        b = np.asarray(model_data['bias'][l])        \n",
    "        h = np.dot(a, W) + b\n",
    "        a = hidden_activation(h)\n",
    "    \n",
    "    W = np.asarray(model_data['weights'][-1]).reshape(model_data['numRows'][-1], model_data['numColumns'][-1], order='F')\n",
    "    b = np.asarray(model_data['bias'][-1])  \n",
    "    h = np.dot(a, W) + b    \n",
    "    a = output_activation(h)\n",
    "\n",
    "    if model_data['output_activation'] == 'softmax':\n",
    "        return np.argmax(a, axis=1)\n",
    "    elif model_data['output_activation'] == 'logistic':\n",
    "        return np.where(a >= threshold, 1, 0).flatten()\n",
    "    else:        \n",
    "        return a.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, with the exported model we perform normalization and prediction for new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:55.027184Z",
     "start_time": "2021-04-20T17:27:55.020283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing data using the exported parameters\n",
    "Xz_test_rec = standard_scaler(X_test, model_data)\n",
    "\n",
    "# Predicting using the exported parameters\n",
    "forward_propagation(Xz_test_rec, model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting a neural network for binary classification data\n",
    "\n",
    "Let us export a neural network model for binary classification using the load_breast_cancer dataset. For the sake of simplicity, let us do all the job in a single cell, i.e., load data, normalize data, training a model and exporting it using `sklearn-export`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:56.220112Z",
     "start_time": "2021-04-20T17:27:55.383558Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data and spliting in train and test sets\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "Xz_train = scaler.fit_transform(X_train)\n",
    "Xz_test = scaler.transform(X_test)\n",
    "\n",
    "# Training a model\n",
    "clf = MLPClassifier(max_iter=10000)\n",
    "clf = clf.fit(Xz_train, y_train)\n",
    "\n",
    "# Exporting the model\n",
    "export = Export([scaler, clf])\n",
    "export.to_json(filename='neural_net_bin.json');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:56.230231Z",
     "start_time": "2021-04-20T17:27:56.222685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('neural_net_bin.json')\n",
    "\n",
    "# Transforming in a dict\n",
    "model_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T16:59:32.470032Z",
     "start_time": "2021-04-20T16:59:32.464090Z"
    }
   },
   "source": [
    "We can reuse our previously methods to predict values using our exported model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:56.243430Z",
     "start_time": "2021-04-20T17:27:56.234111Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict using the exported data\n",
    "Xz_test_rec = standard_scaler(X_test, model_data)\n",
    "forward_propagation(Xz_test_rec, model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting a neural network for regression\n",
    "\n",
    "Another application of neural networks is for regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:56.493082Z",
     "start_time": "2021-04-20T17:27:56.487259Z"
    }
   },
   "outputs": [],
   "source": [
    "# To support old versions of sklearn\n",
    "sklearn_ver = Export.take_sklearn_version()\n",
    "if sklearn_ver[:2] >= (0, 18):\n",
    "    if sklearn_ver[:2] < (0, 24):\n",
    "        from sklearn.neural_network.multilayer_perceptron import MLPRegressor\n",
    "    else:\n",
    "        from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, let us do all the job in a single cell, i.e., load data, normalize data, training a model and exporting it using `sklearn-export`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:28:05.243114Z",
     "start_time": "2021-04-20T17:27:56.852010Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data and spliting in train and test sets\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "Xz_train = scaler.fit_transform(X_train)\n",
    "Xz_test = scaler.transform(X_test)\n",
    "\n",
    "# Training a model\n",
    "reg = MLPRegressor(hidden_layer_sizes=(100, 50, 100), max_iter=5000)\n",
    "reg = reg.fit(Xz_train, y_train)\n",
    "\n",
    "# Exporting the model\n",
    "export = Export([scaler, reg])\n",
    "export.to_json(filename='neural_net_reg.json');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let us load the exported model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:28:05.269348Z",
     "start_time": "2021-04-20T17:28:05.246097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('neural_net_reg.json')\n",
    "\n",
    "# Transforming in a dict\n",
    "model_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we follow the same process to perform normalization and prediction for new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:28:05.290445Z",
     "start_time": "2021-04-20T17:28:05.276361Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([128.73554069, 276.8511989 , 156.12907656, 394.32941984,\n",
       "       188.04028538, 148.24853217, 340.76061632, 175.9700303 ,\n",
       "        98.87510911, 165.34348962,  63.23744282, 223.06276384,\n",
       "       141.45312753, 221.26041875,  34.07921191,  68.78686965,\n",
       "       304.70811266, 293.01082896, 253.81335245, 226.25727015,\n",
       "       217.94542142,  80.29096982,  77.54281424, 208.187663  ,\n",
       "       157.41499781, 130.33654244, 144.75835911, 214.61520285,\n",
       "        53.28400139, 134.50515785, 127.52219077,  72.73341902,\n",
       "       175.68521629, 146.78594745, 194.65089644, 242.32235005,\n",
       "        59.29598215,  77.08796796, 153.90756801,  78.34200464,\n",
       "       129.20914042,  42.96498465, 182.14985805, 188.12840565,\n",
       "       141.4656792 ,  52.55732959,  29.35950781, 151.09608208,\n",
       "       114.74809616, 192.95963684,  87.50361806,  89.55223331,\n",
       "       225.62912315,  91.47201696, 249.26855651,  92.77641067,\n",
       "       164.39929239, 168.89755214, 109.27354306, 150.2296747 ,\n",
       "       204.89958265, 241.43789034,  71.56753227,  62.70045215,\n",
       "       101.18627532, 174.95003313, 240.69920269])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict using the exported data\n",
    "Xz_test_rec = standard_scaler(X_test, model_data)\n",
    "forward_propagation(Xz_test_rec, model_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
