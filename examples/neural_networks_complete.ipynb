{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting more complex models\n",
    "\n",
    "This file contains examples of exporting neural networks from `sklearn` library.\n",
    "\n",
    "### Loading common packages\n",
    "Instaling the `sklearn-export` using pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sklearn_export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading common packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:51.245481Z",
     "start_time": "2021-04-20T17:27:51.241187Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, load_diabetes\n",
    "from sklearn_export import Export\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting a neural network classifier\n",
    "\n",
    "Let us export a more complex model (a neural network for classification) including normalization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:52.038492Z",
     "start_time": "2021-04-20T17:27:52.033367Z"
    }
   },
   "outputs": [],
   "source": [
    "# To support old versions of sklearn\n",
    "sklearn_ver = Export.take_sklearn_version()\n",
    "if sklearn_ver[:2] >= (0, 18):\n",
    "    if sklearn_ver[:2] < (0, 24):\n",
    "        from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
    "    else:\n",
    "        from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firtly, let us load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:52.466640Z",
     "start_time": "2021-04-20T17:27:52.456616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data and spliting in train and test sets\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us normalize the dataset using the Standard Scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:52.847445Z",
     "start_time": "2021-04-20T17:27:52.841498Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "Xz_train = scaler.fit_transform(X_train)\n",
    "Xz_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a neural network for classification using a simple shallow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:53.675312Z",
     "start_time": "2021-04-20T17:27:53.217610Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(max_iter=10000)\n",
    "clf = clf.fit(Xz_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we export the model using `sklearn-export`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:53.684808Z",
     "start_time": "2021-04-20T17:27:53.678197Z"
    }
   },
   "outputs": [],
   "source": [
    "export = Export([scaler, clf])\n",
    "export.to_json(filename='neural_net.json');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the data from te JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:53.944925Z",
     "start_time": "2021-04-20T17:27:53.938898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('neural_net.json')\n",
    "\n",
    "# Transforming in a dict\n",
    "model_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us reuse our standard scaler method from the scaler example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:54.294564Z",
     "start_time": "2021-04-20T17:27:54.289503Z"
    }
   },
   "outputs": [],
   "source": [
    "# An example of a standard scaler implemented with model_data\n",
    "def standard_scaler(X, model_data):\n",
    "    mean = np.asarray(model_data['mean'])\n",
    "    std = np.asarray(model_data['std'])\n",
    "    Xz = (X-mean)/std\n",
    "    return Xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict new values using a trained neural network we need to perform the foward propagation step. It is just a composition of functions (linear combinations followed by activations). The last composition gives the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:54.665686Z",
     "start_time": "2021-04-20T17:27:54.645798Z"
    }
   },
   "outputs": [],
   "source": [
    "# An implementation of the forward propagation process in python numpy\n",
    "def forward_propagation(X, model_data, threshold=0.5):\n",
    "    \n",
    "    # Valid activations in sklearn neural network models\n",
    "    sigmoid = lambda z: 1/(1 + np.exp(-z))\n",
    "    softmax = lambda z: np.exp(z)/np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    relu = lambda z: np.maximum(z, 0)\n",
    "    tanh = lambda z: np.tanh(z)\n",
    "    identity = lambda z: z\n",
    "    \n",
    "    output_activation = None\n",
    "    hidden_activation = None\n",
    "    \n",
    "    # Output activation\n",
    "    if model_data['output_activation'] == 'logistic':\n",
    "        output_activation = sigmoid\n",
    "    elif model_data['output_activation'] == 'softmax':\n",
    "        output_activation = softmax\n",
    "    else:\n",
    "        output_activation = identity        \n",
    "    \n",
    "    # Hidden activation\n",
    "    if model_data['hidden_activation'] == 'sigmoid':\n",
    "        hidden_activation = sigmoid\n",
    "    elif model_data['hidden_activation'] == 'relu':\n",
    "        hidden_activation = relu\n",
    "    elif model_data['hidden_activation'] == 'tanh':\n",
    "        hidden_activation = tanh\n",
    "    else:\n",
    "        hidden_activation = identity\n",
    "    \n",
    "    # Fist layer (is the input data itself)\n",
    "    a = X\n",
    "    \n",
    "    for l in range(len(model_data['weights'])-1):        \n",
    "        W = np.asarray(model_data['weights'][l]).reshape(model_data['numRows'][l], model_data['numColumns'][l], order='F')\n",
    "        b = np.asarray(model_data['bias'][l])        \n",
    "        h = np.dot(a, W) + b\n",
    "        a = hidden_activation(h)\n",
    "    \n",
    "    W = np.asarray(model_data['weights'][-1]).reshape(model_data['numRows'][-1], model_data['numColumns'][-1], order='F')\n",
    "    b = np.asarray(model_data['bias'][-1])  \n",
    "    h = np.dot(a, W) + b    \n",
    "    a = output_activation(h)\n",
    "\n",
    "    if model_data['output_activation'] == 'softmax':\n",
    "        return np.argmax(a, axis=1)\n",
    "    elif model_data['output_activation'] == 'logistic':\n",
    "        return np.where(a >= threshold, 1, 0).flatten()\n",
    "    else:\n",
    "        return a.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, with the exported model we perform normalization and prediction for new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:55.027184Z",
     "start_time": "2021-04-20T17:27:55.020283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalizing data using the exported parameters\n",
    "Xz_test_rec = standard_scaler(X_test, model_data)\n",
    "\n",
    "# Predicting using the exported parameters\n",
    "forward_propagation(Xz_test_rec, model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting a neural network for binary classification data\n",
    "\n",
    "Let us export a neural network model for binary classification using the load_breast_cancer dataset. For the sake of simplicity, let us do all the job in a single cell, i.e., load data, normalize data, training a model and exporting it using `sklearn-export`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:56.220112Z",
     "start_time": "2021-04-20T17:27:55.383558Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data and spliting in train and test sets\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "Xz_train = scaler.fit_transform(X_train)\n",
    "Xz_test = scaler.transform(X_test)\n",
    "\n",
    "# Training a model\n",
    "clf = MLPClassifier(max_iter=10000)\n",
    "clf = clf.fit(Xz_train, y_train)\n",
    "\n",
    "# Exporting the model\n",
    "export = Export([scaler, clf])\n",
    "export.to_json(filename='neural_net_bin.json');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:56.230231Z",
     "start_time": "2021-04-20T17:27:56.222685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('neural_net_bin.json')\n",
    "\n",
    "# Transforming in a dict\n",
    "model_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T16:59:32.470032Z",
     "start_time": "2021-04-20T16:59:32.464090Z"
    }
   },
   "source": [
    "We can reuse our previously methods to predict values using our exported model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:56.243430Z",
     "start_time": "2021-04-20T17:27:56.234111Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict using the exported data\n",
    "Xz_test_rec = standard_scaler(X_test, model_data)\n",
    "forward_propagation(Xz_test_rec, model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting a neural network for regression\n",
    "\n",
    "Another application of neural networks is for regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:27:56.493082Z",
     "start_time": "2021-04-20T17:27:56.487259Z"
    }
   },
   "outputs": [],
   "source": [
    "# To support old versions of sklearn\n",
    "sklearn_ver = Export.take_sklearn_version()\n",
    "if sklearn_ver[:2] >= (0, 18):\n",
    "    if sklearn_ver[:2] < (0, 24):\n",
    "        from sklearn.neural_network.multilayer_perceptron import MLPRegressor\n",
    "    else:\n",
    "        from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, let us do all the job in a single cell, i.e., load data, normalize data, training a model and exporting it using `sklearn-export`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:28:05.243114Z",
     "start_time": "2021-04-20T17:27:56.852010Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load data and spliting in train and test sets\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "\n",
    "# Normalizing data\n",
    "scaler = StandardScaler()\n",
    "Xz_train = scaler.fit_transform(X_train)\n",
    "Xz_test = scaler.transform(X_test)\n",
    "\n",
    "# Training a model\n",
    "reg = MLPRegressor(hidden_layer_sizes=(100, 50, 100), max_iter=5000)\n",
    "reg = clf.fit(Xz_train, y_train)\n",
    "\n",
    "# Exporting the model\n",
    "export = Export([scaler, reg])\n",
    "export.to_json(filename='neural_net_reg.json');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let us load the exported model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:28:05.269348Z",
     "start_time": "2021-04-20T17:28:05.246097Z"
    }
   },
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('neural_net_reg.json')\n",
    "\n",
    "# Transforming in a dict\n",
    "model_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we follow the same process to perform normalization and prediction for new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-20T17:28:05.290445Z",
     "start_time": "2021-04-20T17:28:05.276361Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([155,  90, 133, 167,  31, 104, 183, 127,  15,  31,  20,  76, 122,\n",
       "       199,  15,   5, 198, 121, 149, 119,  57,  54,  66, 134, 123,  79,\n",
       "       119, 171,  20, 118, 186,  28,  34,  81, 109, 127,  15,  13,  98,\n",
       "        54,  11,  26, 153,  47, 189,  54,  17,  97, 122,  89,  73,  29,\n",
       "        17,  11,  87, 123,  11,  90,  55,  20,  73,  57,  22,  25,  51,\n",
       "       159, 108])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict using the exported data\n",
    "Xz_test_rec = standard_scaler(X_test, model_data)\n",
    "forward_propagation(Xz_test_rec, model_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
